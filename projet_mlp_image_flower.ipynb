{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.3.1\n",
      "No GPU found, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset d'entraînement: 816\n",
      "Taille du dataset de test: 204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir une nouvelle transformation pour convertir les images en niveaux de gris\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Redimensionner les images à 112x112\n",
    "    transforms.Grayscale(),  # Convertir les images en niveaux de gris\n",
    "    transforms.ToTensor(),  # Convertir les images en tenseurs PyTorch\n",
    "])\n",
    "\n",
    "\n",
    "# Télécharger et charger le jeu de données Flowers102\n",
    "dataset = datasets.Flowers102(root='./data', \n",
    "                              split='train', \n",
    "                              transform=transform, \n",
    "                              download=True)\n",
    "\n",
    "# Séparer les données en ensemble d'entraînement et de test\n",
    "# Calculer la taille de chaque sous-dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Diviser le dataset en ensemble d'entraînement et de test\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Créer des DataLoader pour les ensembles d'entraînement et de test\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Afficher les tailles des datasets pour vérification\n",
    "print(f'Taille du dataset d\\'entraînement: {len(train_dataset)}')\n",
    "print(f'Taille du dataset de test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoLayerMLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=12544, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): Linear(in_features=100, out_features=102, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(112*112, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 102)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = TwoLayerMLP()\n",
    "print(model)\n",
    "\n",
    "assert len(model.layers) > 0, \"ERROR: You need to write the missing model definition above!\"\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def correct(output, target):\n",
    "    predicted_digits = output.argmax(1)                            # choisir le chiffre avec la plus grande sortie du réseau\n",
    "    correct_ones = (predicted_digits == target).type(torch.float)  # 1.0 pour correct, 0.0 pour incorrect\n",
    "    return correct_ones.sum().item()                               # compter le nombre de bons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Average loss: 4.710788, accuracy: 1.10%\n",
      "Training epoch: 2\n",
      "Average loss: 4.621845, accuracy: 1.10%\n",
      "Training epoch: 3\n",
      "Average loss: 4.620938, accuracy: 0.98%\n",
      "Training epoch: 4\n",
      "Average loss: 4.615074, accuracy: 0.86%\n",
      "Training epoch: 5\n",
      "Average loss: 4.603566, accuracy: 1.59%\n",
      "Training epoch: 6\n",
      "Average loss: 4.590473, accuracy: 1.72%\n",
      "Training epoch: 7\n",
      "Average loss: 4.568855, accuracy: 2.33%\n",
      "Training epoch: 8\n",
      "Average loss: 4.561790, accuracy: 1.96%\n",
      "Training epoch: 9\n",
      "Average loss: 4.535686, accuracy: 2.08%\n",
      "Training epoch: 10\n",
      "Average loss: 4.525084, accuracy: 1.84%\n"
     ]
    }
   ],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    num_items = len(data_loader.dataset)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for data, target in data_loader:\n",
    "        # Copier les données et les cibles sur le GPU\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Effectuer une passe avant\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculer la perte\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Compter le nombre de chiffres corrects\n",
    "        total_correct += correct(output, target)\n",
    "\n",
    "        # Rétropropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = total_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "    print(f\"Average loss: {train_loss:7f}, accuracy: {accuracy:.2%}\")\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch: {epoch+1}\")\n",
    "    train(train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset accuracy: 0.0%, average loss: 4.650013\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(test_loader)\n",
    "    num_items = len(test_loader.dataset)\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Copier les données et les cibles sur le GPU\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Effectuer une passe avant\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculer la perte\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compter le nombre de chiffres corrects\n",
    "            total_correct += correct(output, target)\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%, average loss: {test_loss:>7f}\")\n",
    "\n",
    "test(test_loader, model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
